{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사이킷런을 이용한 특징추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델에 적용할 수 있도록 특징을 뽑아 어떤 값으로 바꿔서 수치화 하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사이킷 런에서 텍스트 데이터를 수치화하는 3가지 방법\n",
    "<tbody></tbody>\n",
    "\n",
    "<li>CountVectorizer - 단순히 각 텍스트에서 횟수를 기준으로 특징을 추출하는 방법</li>\n",
    "<li>TfidVectorizer - TF-IDF 를 사용해 텍스트에서 특징을 추출하는 방법</li>\n",
    "<li>HashingVectorizer - CountVectorizer와 방법은 동일하지만 텍스트를 처리할 때 Hash를 사용하기 떄문에 실행 시간을 크게 줄일 수 있다.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 카운팅은 먼저 단어 사전 벡터를 만들고, 카운팅할 문장을 확인하며 그 단어 사전의 횟수를 카운팅하는 것입니다.\n",
    "# 예를들면 단어사전에\n",
    "# [나는, 너가, 매일, 공부를, 한다, 좋아한다]\n",
    "# 이러한 벡터가 존재한다면,\n",
    "# '나는 매일 공부를 한다'\n",
    "# 라는 문장을 카운팅한다면,\n",
    "# [1,0,1,1,1,0]\n",
    "# 이라는 벡터로 특징을 추출할수 있습니다.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = ['나는 배가 고프다', '내일 점심 뭐먹지', '내일 공부 해야겠다', '점심 먹고 공부 해야지']\n",
    "\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer.fit(text_data)\n",
    "print(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [text_data[0]] # 나는 배가 고프다.\n",
    "print(count_vectorizer.transform(sentence).toarray())\n",
    "\n",
    "#직관적이고 간단해서 여러 상황에서 사용할 수 있다는 장점이 있다.\n",
    "#단순히 회수만을 특징으로 잡기 때문에 큰 의미가 없지만\n",
    "#자주 사용되는 단어들 ex) 조사, 지시대명사 가 높은 특징 값을 가지기 때문에 유의미하게 사용하기 어려울 수 있다.\n",
    "# CountVectorizer의 문제로 인해 이것을 해결할 수 있는 TF-IDF 방식의 특징추출을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF는 특정한 값을 사용해서 텍스트 데이터의 특징을 추출하는 방법\n",
    "# TF(Term Frequency) 란 특정 단어가 하나의 데이터 안에서 등장하는 횟수를 의미한다.\n",
    "# DF(Document Frequency) 란 문서 빈도 값으로, 특정 단어가 여러 데이터에 자주 등장하는지를 알려주는 지표다.\n",
    "# IDF(Inverse Document Frequency)는 이 값에 역수를 취해서 구할 수 있으며 , 특정 단어가 다른 데이터에 등장하지 않을수록 값이 커진다는 것을 의미\n",
    "\n",
    "# 따라서 TF-IDF는 어떤 단어가 해당 문서에 자주 등장하지만 다른 문서에는 많이 없는 단어일수록 높은 값을 가지게 된다.\n",
    "# 그래서 조사나 지시대명사처럼 자주 등장하는 단어는 TF값이 크지만 IDF 값이 작아지므로 CountVectorizer가 가진 문제점을 해결할 수 있다.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_data = ['나는 배가 고프다', '내일 점심 뭐먹지', '내일 공부 해야겠다', '점심 먹고 공부 해야지']\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}\n",
      "\n",
      "\n",
      "['고프다', '공부', '나는', '내일', '먹고', '뭐먹지', '배가', '점심', '해야겠다', '해야지']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer.fit(text_data)\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "print('\\n')\n",
    "print(sorted(tfidf_vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43779123 0.         0.         0.55528266 0.\n",
      "  0.         0.43779123 0.         0.55528266]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [text_data[3]] # ['점심 먹고 공부 해야지']\n",
    "print(tfidf_vectorizer.transform(sentence).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('study')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37119a116a45c2243008a0e89dda34bbd2ff7fcda6a8d95159bf2d6b525d3e84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
